/**
 * DO NOT EDIT THIS FILE!
 * File automatically generated by
 *   /shared/gem5/src/mem/slicc/symbols/StateMachine.py:532
 */

// Created by slicc definition of Module "AMD Hammer-like protocol"

#ifndef __Directory_CONTROLLER_HH__
#define __Directory_CONTROLLER_HH__

#include <iostream>
#include <sstream>
#include <string>

#include "mem/ruby/common/Consumer.hh"
#include "mem/ruby/protocol/TransitionResult.hh"
#include "mem/ruby/protocol/Types.hh"
#include "mem/ruby/slicc_interface/AbstractController.hh"
#include "params/Directory_Controller.hh"

#include "mem/ruby/protocol/Set.hh"
#include "mem/ruby/protocol/TBETable.hh"
namespace gem5
{

namespace ruby
{

extern std::stringstream Directory_transitionComment;

class Directory_Controller : public AbstractController
{
  public:
    typedef Directory_ControllerParams Params;
    Directory_Controller(const Params &p);
    static int getNumControllers();
    void init();

    MessageBuffer *getMandatoryQueue() const;
    MessageBuffer *getMemReqQueue() const;
    MessageBuffer *getMemRespQueue() const;
    void initNetQueues();

    void print(std::ostream& out) const;
    void wakeup();
    void resetStats();
    void regStats();
    void collateStats();

    void recordCacheTrace(int cntrl, CacheRecorder* tr);
    Sequencer* getCPUSequencer() const;
    DMASequencer* getDMASequencer() const;
    GPUCoalescer* getGPUCoalescer() const;

    bool functionalReadBuffers(PacketPtr&);
    bool functionalReadBuffers(PacketPtr&, WriteMask&);
    int functionalWriteBuffers(PacketPtr&);

    void countTransition(Directory_State state, Directory_Event event);
    void possibleTransition(Directory_State state, Directory_Event event);
    uint64_t getEventCount(Directory_Event event);
    bool isPossible(Directory_State state, Directory_Event event);
    uint64_t getTransitionCount(Directory_State state, Directory_Event event);

private:
    DirectoryMemory* m_directory_ptr;
    CacheMemory* m_probeFilter_ptr;
    Cycles m_from_memory_controller_latency;
    Cycles m_to_memory_controller_latency;
    bool m_probe_filter_enabled;
    bool m_full_bit_dir_enabled;
    MessageBuffer* m_forwardFromDir_ptr;
    MessageBuffer* m_responseFromDir_ptr;
    MessageBuffer* m_dmaResponseFromDir_ptr;
    MessageBuffer* m_unblockToDir_ptr;
    MessageBuffer* m_responseToDir_ptr;
    MessageBuffer* m_requestToDir_ptr;
    MessageBuffer* m_dmaRequestToDir_ptr;
    MessageBuffer* m_triggerQueue_ptr;
    MessageBuffer* m_requestToMemory_ptr;
    MessageBuffer* m_responseFromMemory_ptr;
    TransitionResult doTransition(Directory_Event event,
                                  Directory_PfEntry* m_cache_entry_ptr,
                                  Directory_TBE* m_tbe_ptr,
                                  Addr addr);

    TransitionResult doTransitionWorker(Directory_Event event,
                                        Directory_State state,
                                        Directory_State& next_state,
                                        Directory_TBE*& m_tbe_ptr,
                                        Directory_PfEntry*& m_cache_entry_ptr,
                                        Addr addr);

    Directory_Event m_curTransitionEvent;
    Directory_State m_curTransitionNextState;

    Directory_Event curTransitionEvent() { return m_curTransitionEvent; }
    Directory_State curTransitionNextState() { return m_curTransitionNextState; }

    int m_counters[Directory_State_NUM][Directory_Event_NUM];
    int m_event_counters[Directory_Event_NUM];
    bool m_possible[Directory_State_NUM][Directory_Event_NUM];

    static std::vector<statistics::Vector *> eventVec;
    static std::vector<std::vector<statistics::Vector *> > transVec;
    static int m_num_controllers;

    // Internal functions
    Directory_Entry* getDirectoryEntry(const Addr& param_addr);
    Directory_PfEntry* getProbeFilterEntry(const Addr& param_addr);
    Directory_State getState(Directory_TBE* param_tbe, Directory_PfEntry* param_pf_entry, const Addr& param_addr);
    void setState(Directory_TBE* param_tbe, Directory_PfEntry* param_pf_entry, const Addr& param_addr, const Directory_State& param_state);
    AccessPermission getAccessPermission(const Addr& param_addr);
    void setAccessPermission(Directory_PfEntry* param_pf_entry, const Addr& param_addr, const Directory_State& param_state);
    void functionalRead(const Addr& param_addr, Packet* param_pkt);
    int functionalWrite(const Addr& param_addr, Packet* param_pkt);
    Directory_Event cache_request_to_event(const CoherenceRequestType& param_type);

    // Set and Reset for cache_entry variable
    void set_cache_entry(Directory_PfEntry*& m_cache_entry_ptr, AbstractCacheEntry* m_new_cache_entry);
    void unset_cache_entry(Directory_PfEntry*& m_cache_entry_ptr);

    // Set and Reset for tbe variable
    void set_tbe(Directory_TBE*& m_tbe_ptr, Directory_TBE* m_new_tbe);
    void unset_tbe(Directory_TBE*& m_tbe_ptr);

    // Actions
    /** \brief manually set the MRU bit for pf entry */
    void r_setMRU(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief assert unblocker not owner */
    void auno_assertUnblockerNotOwner(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief update owner */
    void uo_updateOwnerIfPf(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief update sharer if full-bit directory */
    void us_updateSharerIfFBD(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send writeback ack to requestor */
    void a_sendWriteBackAck(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send block ack to the owner */
    void oc_sendBlockAck(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send writeback nack to requestor */
    void b_sendWriteBackNack(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Allocate ProbeFilterEntry */
    void pfa_probeFilterAllocate(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate ProbeFilterEntry */
    void pfd_probeFilterDeallocate(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate ProbeFilterEntry */
    void ppfd_possibleProbeFilterDeallocate(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Allocate TBE */
    void v_allocateTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Record Data in TBE */
    void vd_allocateDmaRequestInTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief set pending msgs to all */
    void pa_setPendingMsgsToAll(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief set pending msgs to one */
    void po_setPendingMsgsToOne(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate TBE */
    void w_deallocateTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forwarded request, set the ack amount to one */
    void sa_setAcksToOne(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Non-forwarded request, set the ack amount to all */
    void saa_setAcksToAllIfPF(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Decrement the number of messages for which we're waiting */
    void m_decrementNumberOfMessages(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Decrement the number of messages for which we're waiting */
    void mu_decrementNumberOfUnblocks(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop response queue */
    void n_popResponseQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Check if we have received all the messages required for completion */
    void o_checkForCompletion(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Check for merged GETS completion */
    void os_checkForMergedGetSCompletion(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Set pending messages to waiting sharers */
    void sp_setPendingMsgsToMergedSharers(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief if probe filter, no need to wait for acks */
    void spa_setPendingAcksToZeroIfPF(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief indicate that we should skip waiting for cpu acks */
    void sc_signalCompletionIfPF(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data to requestor */
    void d_sendData(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send Data to DMA controller from memory */
    void dr_sendDmaData(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send Data to DMA controller from tbe */
    void dt_sendDmaDataFromTbe(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send Ack to DMA controller */
    void da_sendDmaAck(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Record Exclusive in TBE */
    void rx_recordExclusiveInTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Record Data in TBE */
    void r_recordDataInTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Record GETS requestor in TBE */
    void rs_recordGetSRequestor(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief We saw other sharers */
    void r_setSharerBit(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief We saw other sharers */
    void so_setOwnerBit(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Queue off-chip fetch request */
    void qf_queueMemoryFetchRequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Queue off-chip fetch request */
    void qd_queueMemoryRequestFromDmaRead(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward requests if necessary */
    void fn_forwardRequestIfNecessary(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief invalidate all copies */
    void ia_invalidateAllRequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief invalidate all copies */
    void io_invalidateOwnerRequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward requests to all nodes */
    void fb_forwardRequestBcast(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward coalesced read request to owner */
    void fr_forwardMergeReadRequestsToOwner(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward request to one or more nodes */
    void fc_forwardRequestConditionalOwner(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward request to one or more nodes if the requestor is not the owner */
    void nofc_forwardRequestConditionalOwner(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward requests */
    void f_forwardWriteFromDma(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Forward requests */
    void f_forwardReadFromDma(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop incoming request queue */
    void i_popIncomingRequestQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop incoming unblock queue */
    void j_popIncomingUnblockQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief wake-up dependents */
    void k_wakeUpDependents(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop off-chip request queue */
    void l_popMemQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop trigger queue */
    void g_popTriggerQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief pop dma request queue */
    void p_popDmaRequestQueue(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Stall and wait the dma request queue */
    void zd_stallAndWaitDMARequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief record data from memory to TBE */
    void r_recordMemoryData(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief record data from cache response to TBE */
    void r_recordCacheData(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Assert that a cache provided the data */
    void a_assertCacheData(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Assert that request is not current owner */
    void ano_assertNotOwner(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Assert that request is not a current sharer */
    void ans_assertNotSharer(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief remove current sharer */
    void rs_removeSharer(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief clear current sharers */
    void cs_clearSharers(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write PUTX data to memory */
    void l_queueMemoryWBRequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write DMA data to memory */
    void ld_queueMemoryDmaWrite(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write data to memory from TBE */
    void ly_queueMemoryWriteFromTBE(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Check PUTX/PUTO response message */
    void ll_checkIncomingWriteback(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);
    /** \brief Recycle the request queue */
    void z_stallAndWaitRequest(Directory_TBE*& m_tbe_ptr, Directory_PfEntry*& m_cache_entry_ptr, Addr addr);

    // Objects
    Set* m_fwd_set_ptr;
    TBETable<Directory_TBE>* m_TBEs_ptr;
};

} // namespace ruby
} // namespace gem5

#endif // __Directory_CONTROLLER_H__
