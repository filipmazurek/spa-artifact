/**
 * DO NOT EDIT THIS FILE!
 * File automatically generated by
 *   /shared/gem5/src/mem/slicc/symbols/StateMachine.py:532
 */

// Created by slicc definition of Module "AMD Hammer-like protocol"

#ifndef __L1Cache_CONTROLLER_HH__
#define __L1Cache_CONTROLLER_HH__

#include <iostream>
#include <sstream>
#include <string>

#include "mem/ruby/common/Consumer.hh"
#include "mem/ruby/protocol/TransitionResult.hh"
#include "mem/ruby/protocol/Types.hh"
#include "mem/ruby/slicc_interface/AbstractController.hh"
#include "params/L1Cache_Controller.hh"

#include "mem/ruby/protocol/TBETable.hh"
namespace gem5
{

namespace ruby
{

extern std::stringstream L1Cache_transitionComment;

class L1Cache_Controller : public AbstractController
{
  public:
    typedef L1Cache_ControllerParams Params;
    L1Cache_Controller(const Params &p);
    static int getNumControllers();
    void init();

    MessageBuffer *getMandatoryQueue() const;
    MessageBuffer *getMemReqQueue() const;
    MessageBuffer *getMemRespQueue() const;
    void initNetQueues();

    void print(std::ostream& out) const;
    void wakeup();
    void resetStats();
    void regStats();
    void collateStats();

    void recordCacheTrace(int cntrl, CacheRecorder* tr);
    Sequencer* getCPUSequencer() const;
    DMASequencer* getDMASequencer() const;
    GPUCoalescer* getGPUCoalescer() const;

    bool functionalReadBuffers(PacketPtr&);
    bool functionalReadBuffers(PacketPtr&, WriteMask&);
    int functionalWriteBuffers(PacketPtr&);

    void countTransition(L1Cache_State state, L1Cache_Event event);
    void possibleTransition(L1Cache_State state, L1Cache_Event event);
    uint64_t getEventCount(L1Cache_Event event);
    bool isPossible(L1Cache_State state, L1Cache_Event event);
    uint64_t getTransitionCount(L1Cache_State state, L1Cache_Event event);

private:
    Sequencer* m_sequencer_ptr;
    CacheMemory* m_L1Icache_ptr;
    CacheMemory* m_L1Dcache_ptr;
    CacheMemory* m_L2cache_ptr;
    Cycles m_cache_response_latency;
    Cycles m_issue_latency;
    Cycles m_l2_cache_hit_latency;
    bool m_no_mig_atomic;
    bool m_send_evictions;
    MessageBuffer* m_requestFromCache_ptr;
    MessageBuffer* m_responseFromCache_ptr;
    MessageBuffer* m_unblockFromCache_ptr;
    MessageBuffer* m_forwardToCache_ptr;
    MessageBuffer* m_responseToCache_ptr;
    MessageBuffer* m_mandatoryQueue_ptr;
    MessageBuffer* m_triggerQueue_ptr;
    TransitionResult doTransition(L1Cache_Event event,
                                  L1Cache_Entry* m_cache_entry_ptr,
                                  L1Cache_TBE* m_tbe_ptr,
                                  Addr addr);

    TransitionResult doTransitionWorker(L1Cache_Event event,
                                        L1Cache_State state,
                                        L1Cache_State& next_state,
                                        L1Cache_TBE*& m_tbe_ptr,
                                        L1Cache_Entry*& m_cache_entry_ptr,
                                        Addr addr);

    L1Cache_Event m_curTransitionEvent;
    L1Cache_State m_curTransitionNextState;

    L1Cache_Event curTransitionEvent() { return m_curTransitionEvent; }
    L1Cache_State curTransitionNextState() { return m_curTransitionNextState; }

    int m_counters[L1Cache_State_NUM][L1Cache_Event_NUM];
    int m_event_counters[L1Cache_Event_NUM];
    bool m_possible[L1Cache_State_NUM][L1Cache_Event_NUM];

    static std::vector<statistics::Vector *> eventVec;
    static std::vector<std::vector<statistics::Vector *> > transVec;
    static int m_num_controllers;

    // Internal functions
    L1Cache_Entry* getCacheEntry(const Addr& param_address);
    void functionalRead(const Addr& param_addr, Packet* param_pkt);
    int functionalWrite(const Addr& param_addr, Packet* param_pkt);
    L1Cache_Entry* getL2CacheEntry(const Addr& param_address);
    L1Cache_Entry* getL1DCacheEntry(const Addr& param_address);
    L1Cache_Entry* getL1ICacheEntry(const Addr& param_address);
    L1Cache_State getState(L1Cache_TBE* param_tbe, L1Cache_Entry* param_cache_entry, const Addr& param_addr);
    void setState(L1Cache_TBE* param_tbe, L1Cache_Entry* param_cache_entry, const Addr& param_addr, const L1Cache_State& param_state);
    AccessPermission getAccessPermission(const Addr& param_addr);
    void setAccessPermission(L1Cache_Entry* param_cache_entry, const Addr& param_addr, const L1Cache_State& param_state);
    L1Cache_Event mandatory_request_type_to_event(const RubyRequestType& param_type);
    MachineType testAndClearLocalHit(L1Cache_Entry* param_cache_entry);
    bool IsAtomicAccessed(L1Cache_Entry* param_cache_entry);

    // Set and Reset for cache_entry variable
    void set_cache_entry(L1Cache_Entry*& m_cache_entry_ptr, AbstractCacheEntry* m_new_cache_entry);
    void unset_cache_entry(L1Cache_Entry*& m_cache_entry_ptr);

    // Set and Reset for tbe variable
    void set_tbe(L1Cache_TBE*& m_tbe_ptr, L1Cache_TBE* m_new_tbe);
    void unset_tbe(L1Cache_TBE*& m_tbe_ptr);

    // Actions
    /** \brief Issue GETS */
    void a_issueGETS(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Issue GETX */
    void b_issueGETX(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Issue GETX */
    void b_issueGETXIfMoreThanOne(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Issue GETF */
    void bf_issueGETF(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send exclusive data from cache to requestor */
    void c_sendExclusiveData(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send exclusive data from tbe to requestor */
    void ct_sendExclusiveDataFromTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Issue PUT */
    void d_issuePUT(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Issue PUTF */
    void df_issuePUTF(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from cache to requestor */
    void e_sendData(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from cache to requestor, remaining the owner */
    void ee_sendDataShared(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from TBE to requestor, keep a shared copy */
    void et_sendDataSharedFromTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from cache to all requestors, still the owner */
    void em_sendDataSharedMultiple(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from tbe to all requestors */
    void emt_sendDataSharedMultipleFromTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send ack from cache to requestor */
    void f_sendAck(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send shared ack from cache to requestor */
    void ff_sendAckShared(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send unblock to memory */
    void g_sendUnblock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send unblock to memory and indicate M/O/E state */
    void gm_sendUnblockM(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send unblock to memory and indicate S state */
    void gs_sendUnblockS(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Notify sequencer the load completed. */
    void h_load_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Notify sequencer the ifetch completed. */
    void h_ifetch_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief load required external msgs */
    void hx_external_load_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Notify sequencer that store completed. */
    void hh_store_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Notify sequencer that flush completed. */
    void hh_flush_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief store required external msgs. */
    void sx_external_store_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief store required external msgs. */
    void sxt_trig_ext_store_hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Allocate TBE */
    void i_allocateTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Allocate TBE */
    void it_allocateTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop trigger queue. */
    void j_popTriggerQueue(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop mandatory queue. */
    void k_popMandatoryQueue(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop forwareded request queue. */
    void l_popForwardQueue(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Copy data from TBE to L2 cache entry. */
    void hp_copyFromTBEToL2(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Copy data from TBE to L1 cache entry. */
    void nb_copyFromTBEToL1(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Decrement the number of messages for which we're waiting */
    void m_decrementNumberOfMessages(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief When moving SS state, update current owner. */
    void uo_updateCurrentOwner(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Pop response queue */
    void n_popResponseQueue(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief  */
    void ll_L2toL1Transfer(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Check if we have received all the messages required for completion */
    void o_checkForCompletion(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Decrement the number of messages for which we're waiting by one */
    void p_decrementNumberOfMessagesByOne(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Increment the number of messages for which we're waiting by one */
    void pp_incrementNumberOfMessagesByOne(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from TBE to cache */
    void q_sendDataFromTBEToCache(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send shared data from TBE to cache, still the owner */
    void sq_sendSharedDataFromTBEToCache(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from TBE to cache, multiple sharers, still the owner */
    void qm_sendDataFromTBEToCache(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send data from TBE to memory */
    void qq_sendDataFromTBEToMemory(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief We saw other sharers */
    void r_setSharerBit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate TBE */
    void s_deallocateTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send exclusive data from TBE to memory */
    void t_sendExclusiveDataFromTBEToMemory(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write data to cache */
    void u_writeDataToCache(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write data to TBE */
    void uf_writeDataToCacheTBE(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write data to cache, assert it was same as before */
    void v_writeDataToCacheVerify(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Write data to TBE, assert it was same as before */
    void vt_writeDataToTBEVerify(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate cache block.  Sets the cache to invalid, allowing a replacement in parallel with a fetch. */
    void gg_deallocateL1CacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Set L1 D-cache tag equal to tag of block B. */
    void ii_allocateL1DCacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Set L1 I-cache tag equal to tag of block B. */
    void jj_allocateL1ICacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Set L2 cache tag equal to tag of block B. */
    void vv_allocateL2CacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate L2 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch. */
    void rr_deallocateL2CacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Deallocate an L1 or L2 cache block. */
    void gr_deallocateCacheBlock(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief sends eviction information to the processor */
    void forward_eviction_to_cpu(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand miss */
    void uu_profileL1DataMiss(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand hits */
    void uu_profileL1DataHit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand miss */
    void uu_profileL1InstMiss(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand hits */
    void uu_profileL1InstHit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand miss */
    void uu_profileL2Miss(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Profile the demand hits */
    void uu_profileL2Hit(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief Send the head of the mandatory queue to the back of the queue. */
    void zz_stallAndWaitMandatoryQueue(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief stall */
    void z_stall(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief wake-up dependents */
    void kd_wakeUpDependents(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);
    /** \brief wake-up all dependents */
    void ka_wakeUpAllDependents(L1Cache_TBE*& m_tbe_ptr, L1Cache_Entry*& m_cache_entry_ptr, Addr addr);

    // Objects
    TBETable<L1Cache_TBE>* m_TBEs_ptr;
};

} // namespace ruby
} // namespace gem5

#endif // __L1Cache_CONTROLLER_H__
